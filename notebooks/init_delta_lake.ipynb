{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook is for writing a delta lake containing a single empty table to S3. Before running the cell below, confirm the version of hadoop:\n",
    "\n",
    "1. In a terminal, run `poetry shell` in the directory for this project. \n",
    "2. `pyspark`\n",
    "3. Retrieve the hadoop version\n",
    "\n",
    "    ```python\n",
    "    spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()\n",
    "    ```\n",
    "\n",
    "4. Replace the hadoop version appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "HADOOP_VERSION='3.3.4'\n",
    "\n",
    "credentials = boto3.Session(profile_name='default').get_credentials()\n",
    "\n",
    "# Upload a delta lake file to S3\n",
    "builder = SparkSession.builder.appName('s3-upload') \\\n",
    "    .config('spark.jars.packages', f'org.apache.hadoop:hadoop-aws:{HADOOP_VERSION}') \\\n",
    "    .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .config('spark.hadoop.fs.s3a.access.key', credentials.access_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.secret.key', credentials.secret_key) \\\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "\n",
    "# Extra package will be downloaded to ~/.ivy2/jars\n",
    "extra_packages = [f'org.apache.hadoop:hadoop-aws:{HADOOP_VERSION}']\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder, extra_packages=extra_packages).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table with the following columns and types.\n",
    "\n",
    "| Name | Type | \n",
    "| --- | --- |\n",
    "| Year | bigint |\n",
    "| Position | string |\n",
    "| Player | string |\n",
    "| FantasyPoints | double |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Player\", StringType(), True),\n",
    "    StructField(\"FantasyPoints\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame([], schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the table in delta lake format to s3. Set the value for `BUCKET_NAME` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=''\n",
    "\n",
    "df.write.format('delta').mode('overwrite').save(f's3a://{BUCKET_NAME}/database/top_performers_delta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
