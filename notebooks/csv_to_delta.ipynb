{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook represents a glue job that reads .CSV files from S3, converts them into Spark DataFrames and performs transformations on those DataFrames before writing the frames as a table in the delta lake. To run the notebook locally, comment out the additional *script_location* and *temp_dir* configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 4.0\n",
    "%number_of_workers 2\n",
    "%worker_type G.1X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"datalake-formats\": \"delta\",\n",
    "    \n",
    "    # Replace BUCKET_NAME with the name of your S3 bucket\n",
    "    # Comment out these lines if running the notebook locally\n",
    "    \n",
    "    \"script_location\": \"s3://your-name-delta-lake-project-1/scripts/\",\n",
    "    \"temp_dir\": \"s3://your-name-delta-lake-project-1/scripts/temp/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "\n",
    "from pyspark.sql.functions import col, lit, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = glueContext.create_data_frame.from_catalog(\n",
    "    database=\"delta_lake\",\n",
    "    table_name=\"top_performers_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_resource = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through files in an s3 folder with boto3\n",
    "bucket = s3_resource.Bucket(f'{BUCKET_NAME}')\n",
    "\n",
    "for obj in bucket.objects.filter(Prefix='database/raw/', ):\n",
    "\n",
    "    if not obj.key.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    # Get year from file name\n",
    "    year = int(obj.key.split('/')[2].split('.')[0])\n",
    "\n",
    "    # Read csv file from s3 and convert to DataFrame\n",
    "    df = glueContext.create_data_frame.from_options(\n",
    "        connection_type='s3',\n",
    "        connection_options={'paths': [f's3a://{BUCKET_NAME}/{obj.key}']},\n",
    "        format='csv',\n",
    "        format_options={'withHeader': True},\n",
    "        transformation_ctx='datasource0')\n",
    "    \n",
    "    df = df.withColumn(\"FantasyPoints\", df[\"FantasyPoints\"].cast(\"double\"))\n",
    "    \n",
    "    # Create a window specification to get top 12 players by position\n",
    "    # Note: This will actually return all players that have a top 12 fantasy score\n",
    "    # which means that there may be more than 12 players for a given position\n",
    "    windowSpec = Window.partitionBy('Pos').orderBy(df['FantasyPoints'].desc())\n",
    "    \n",
    "    df_csv = df.filter(col(\"Pos\").isin([\"QB\", \"WR\", \"RB\"])) \\\n",
    "        .select('Pos', 'Player', 'FantasyPoints') \\\n",
    "        .withColumn(\"rank\", rank().over(windowSpec)) \\\n",
    "        .filter(col(\"rank\") <= 12)\n",
    "    \n",
    "    # Clean up additional columns and add year\n",
    "    df_csv = df_csv.withColumnRenamed(\"Pos\", \"Position\") \\\n",
    "        .withColumn(\"Year\", lit(year)) \\\n",
    "        .select(\"Year\", \"Position\", \"Player\", \"FantasyPoints\")\n",
    "\n",
    "    df_total = df_total.union(df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write pyspark dataframe to delta lake\n",
    "df_total.write.format(\"delta\").mode(\"overwrite\").save(f\"s3a://{BUCKET_NAME}/database/top_performers_delta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
